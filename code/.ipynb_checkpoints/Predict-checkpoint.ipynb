{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-04-26T12:09:06.595727Z",
     "iopub.status.busy": "2022-04-26T12:09:06.595338Z",
     "iopub.status.idle": "2022-04-26T12:09:12.963729Z",
     "shell.execute_reply": "2022-04-26T12:09:12.96286Z",
     "shell.execute_reply.started": "2022-04-26T12:09:06.59567Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2022-04-26T12:09:12.965925Z",
     "iopub.status.busy": "2022-04-26T12:09:12.965594Z",
     "iopub.status.idle": "2022-04-26T12:09:12.980682Z",
     "shell.execute_reply": "2022-04-26T12:09:12.979921Z",
     "shell.execute_reply.started": "2022-04-26T12:09:12.965827Z"
    }
   },
   "outputs": [],
   "source": [
    "# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB --> {:.2f} MB (Decreased by {:.1f}%)'.format(\n",
    "        start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and show data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-26T12:09:12.982522Z",
     "iopub.status.busy": "2022-04-26T12:09:12.981971Z",
     "iopub.status.idle": "2022-04-26T12:09:12.99351Z",
     "shell.execute_reply": "2022-04-26T12:09:12.992555Z",
     "shell.execute_reply.started": "2022-04-26T12:09:12.982449Z"
    }
   },
   "outputs": [],
   "source": [
    "def state(message,start = True, time = 0):\n",
    "    if(start):\n",
    "        print(f'Working on {message} ... ')\n",
    "    else :\n",
    "        print(f'Working on {message} took ({round(time , 3)}) Sec \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-26T12:09:12.995392Z",
     "iopub.status.busy": "2022-04-26T12:09:12.994892Z",
     "iopub.status.idle": "2022-04-26T12:09:47.075223Z",
     "shell.execute_reply": "2022-04-26T12:09:47.074342Z",
     "shell.execute_reply.started": "2022-04-26T12:09:12.995205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 983.90 MB --> 339.28 MB (Decreased by 65.5%)\n",
      "Memory usage of dataframe is 413.18 MB --> 140.19 MB (Decreased by 66.1%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assists</th>\n",
       "      <th>boosts</th>\n",
       "      <th>damageDealt</th>\n",
       "      <th>DBNOs</th>\n",
       "      <th>headshotKills</th>\n",
       "      <th>heals</th>\n",
       "      <th>killPlace</th>\n",
       "      <th>killPoints</th>\n",
       "      <th>kills</th>\n",
       "      <th>killStreaks</th>\n",
       "      <th>...</th>\n",
       "      <th>revives</th>\n",
       "      <th>rideDistance</th>\n",
       "      <th>roadKills</th>\n",
       "      <th>swimDistance</th>\n",
       "      <th>teamKills</th>\n",
       "      <th>vehicleDestroys</th>\n",
       "      <th>walkDistance</th>\n",
       "      <th>weaponsAcquired</th>\n",
       "      <th>winPoints</th>\n",
       "      <th>winPlacePerc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446965e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.338149e-01</td>\n",
       "      <td>1.106908e+00</td>\n",
       "      <td>1.307172e+02</td>\n",
       "      <td>6.578755e-01</td>\n",
       "      <td>2.268196e-01</td>\n",
       "      <td>1.370147e+00</td>\n",
       "      <td>4.759935e+01</td>\n",
       "      <td>5.050060e+02</td>\n",
       "      <td>9.247833e-01</td>\n",
       "      <td>5.439551e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.646590e-01</td>\n",
       "      <td>6.061160e+02</td>\n",
       "      <td>3.496091e-03</td>\n",
       "      <td>4.509323e+00</td>\n",
       "      <td>2.386841e-02</td>\n",
       "      <td>7.918208e-03</td>\n",
       "      <td>1.154218e+03</td>\n",
       "      <td>3.660488e+00</td>\n",
       "      <td>6.064601e+02</td>\n",
       "      <td>4.728218e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.885731e-01</td>\n",
       "      <td>1.715794e+00</td>\n",
       "      <td>1.707806e+02</td>\n",
       "      <td>1.145743e+00</td>\n",
       "      <td>6.021553e-01</td>\n",
       "      <td>2.679982e+00</td>\n",
       "      <td>2.746294e+01</td>\n",
       "      <td>6.275049e+02</td>\n",
       "      <td>1.558445e+00</td>\n",
       "      <td>7.109721e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>4.721671e-01</td>\n",
       "      <td>1.498344e+03</td>\n",
       "      <td>7.337297e-02</td>\n",
       "      <td>3.050220e+01</td>\n",
       "      <td>1.673935e-01</td>\n",
       "      <td>9.261157e-02</td>\n",
       "      <td>1.183497e+03</td>\n",
       "      <td>2.456544e+00</td>\n",
       "      <td>7.397004e+02</td>\n",
       "      <td>3.074050e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.400000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.551000e+02</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.424000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.700000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.856000e+02</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.583000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.860000e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>7.100000e+01</td>\n",
       "      <td>1.172000e+03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.909750e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.976000e+03</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.495000e+03</td>\n",
       "      <td>7.407000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.200000e+01</td>\n",
       "      <td>3.300000e+01</td>\n",
       "      <td>6.616000e+03</td>\n",
       "      <td>5.300000e+01</td>\n",
       "      <td>6.400000e+01</td>\n",
       "      <td>8.000000e+01</td>\n",
       "      <td>1.010000e+02</td>\n",
       "      <td>2.170000e+03</td>\n",
       "      <td>7.200000e+01</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.900000e+01</td>\n",
       "      <td>4.071000e+04</td>\n",
       "      <td>1.800000e+01</td>\n",
       "      <td>3.823000e+03</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>2.578000e+04</td>\n",
       "      <td>2.360000e+02</td>\n",
       "      <td>2.013000e+03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            assists        boosts   damageDealt         DBNOs  headshotKills  \\\n",
       "count  4.446966e+06  4.446966e+06  4.446966e+06  4.446966e+06   4.446966e+06   \n",
       "mean   2.338149e-01  1.106908e+00  1.307172e+02  6.578755e-01   2.268196e-01   \n",
       "std    5.885731e-01  1.715794e+00  1.707806e+02  1.145743e+00   6.021553e-01   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   0.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00  8.424000e+01  0.000000e+00   0.000000e+00   \n",
       "75%    0.000000e+00  2.000000e+00  1.860000e+02  1.000000e+00   0.000000e+00   \n",
       "max    2.200000e+01  3.300000e+01  6.616000e+03  5.300000e+01   6.400000e+01   \n",
       "\n",
       "              heals     killPlace    killPoints         kills   killStreaks  \\\n",
       "count  4.446966e+06  4.446966e+06  4.446966e+06  4.446966e+06  4.446966e+06   \n",
       "mean   1.370147e+00  4.759935e+01  5.050060e+02  9.247833e-01  5.439551e-01   \n",
       "std    2.679982e+00  2.746294e+01  6.275049e+02  1.558445e+00  7.109721e-01   \n",
       "min    0.000000e+00  1.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  2.400000e+01  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    0.000000e+00  4.700000e+01  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%    2.000000e+00  7.100000e+01  1.172000e+03  1.000000e+00  1.000000e+00   \n",
       "max    8.000000e+01  1.010000e+02  2.170000e+03  7.200000e+01  2.000000e+01   \n",
       "\n",
       "       ...       revives  rideDistance     roadKills  swimDistance  \\\n",
       "count  ...  4.446966e+06  4.446966e+06  4.446966e+06  4.446966e+06   \n",
       "mean   ...  1.646590e-01  6.061160e+02  3.496091e-03  4.509323e+00   \n",
       "std    ...  4.721671e-01  1.498344e+03  7.337297e-02  3.050220e+01   \n",
       "min    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%    ...  0.000000e+00  1.909750e-01  0.000000e+00  0.000000e+00   \n",
       "max    ...  3.900000e+01  4.071000e+04  1.800000e+01  3.823000e+03   \n",
       "\n",
       "          teamKills  vehicleDestroys  walkDistance  weaponsAcquired  \\\n",
       "count  4.446966e+06     4.446966e+06  4.446966e+06     4.446966e+06   \n",
       "mean   2.386841e-02     7.918208e-03  1.154218e+03     3.660488e+00   \n",
       "std    1.673935e-01     9.261157e-02  1.183497e+03     2.456544e+00   \n",
       "min    0.000000e+00     0.000000e+00  0.000000e+00     0.000000e+00   \n",
       "25%    0.000000e+00     0.000000e+00  1.551000e+02     2.000000e+00   \n",
       "50%    0.000000e+00     0.000000e+00  6.856000e+02     3.000000e+00   \n",
       "75%    0.000000e+00     0.000000e+00  1.976000e+03     5.000000e+00   \n",
       "max    1.200000e+01     5.000000e+00  2.578000e+04     2.360000e+02   \n",
       "\n",
       "          winPoints  winPlacePerc  \n",
       "count  4.446966e+06  4.446965e+06  \n",
       "mean   6.064601e+02  4.728218e-01  \n",
       "std    7.397004e+02  3.074050e-01  \n",
       "min    0.000000e+00  0.000000e+00  \n",
       "25%    0.000000e+00  2.000000e-01  \n",
       "50%    0.000000e+00  4.583000e-01  \n",
       "75%    1.495000e+03  7.407000e-01  \n",
       "max    2.013000e+03  1.000000e+00  \n",
       "\n",
       "[8 rows x 25 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataset\n",
    "df_train = pd.read_csv('../input/train_V2.csv')\n",
    "df_test = pd.read_csv('../input/test_V2.csv')\n",
    "\n",
    "# Reduce memory use\n",
    "df_train=reduce_mem_usage(df_train)\n",
    "df_test=reduce_mem_usage(df_test)\n",
    "\n",
    "# Show some data\n",
    "df_train.head()\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-26T12:09:47.078981Z",
     "iopub.status.busy": "2022-04-26T12:09:47.078712Z",
     "iopub.status.idle": "2022-04-26T12:09:48.290097Z",
     "shell.execute_reply": "2022-04-26T12:09:48.289264Z",
     "shell.execute_reply.started": "2022-04-26T12:09:47.078924Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2      768836\n",
       "3      768347\n",
       "4      689622\n",
       "1      580951\n",
       "5      540721\n",
       "        ...  \n",
       "77          1\n",
       "75          1\n",
       "74          1\n",
       "71          1\n",
       "236         1\n",
       "Name: weaponsAcquired, Length: 97, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reference link: https://www.kaggle.com/melonaded/a-beginner-guide-to-top-35-lasso-rf-lgbm\n",
    "\n",
    "# Drop features\n",
    "df_train = df_train.drop(['longestKill', 'numGroups'], axis=1)\n",
    "df_test = df_test.drop(['longestKill', 'numGroups'], axis=1)\n",
    "\n",
    "# Check row with NaN value\n",
    "df_train[df_train['winPlacePerc'].isnull()]\n",
    "# Drop row with NaN 'winPlacePerc' value\n",
    "df_train.drop(2744604, inplace=True)\n",
    "\n",
    "df_train['kills'].value_counts()\n",
    "df_train['DBNOs'].value_counts()\n",
    "df_train['weaponsAcquired'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-26T12:09:48.292767Z",
     "iopub.status.busy": "2022-04-26T12:09:48.292465Z",
     "iopub.status.idle": "2022-04-26T12:09:48.319923Z",
     "shell.execute_reply": "2022-04-26T12:09:48.319156Z",
     "shell.execute_reply.started": "2022-04-26T12:09:48.292717Z"
    }
   },
   "outputs": [],
   "source": [
    "def feature_engineering(df,is_train=True):\n",
    "    if is_train: \n",
    "        df = df[df['maxPlace'] > 1]\n",
    "\n",
    "    state('totalDistance')\n",
    "    s = timer()\n",
    "    df['totalDistance'] = df['rideDistance'] + df[\"walkDistance\"] + df[\"swimDistance\"]\n",
    "    e = timer()\n",
    "    state('totalDistance', False, e - s)\n",
    "          \n",
    "    state('killPlace_over_maxPlace')\n",
    "    s = timer()\n",
    "    df['killPlace_over_maxPlace'] = df['killPlace'] / df['maxPlace']\n",
    "    e = timer()                                  \n",
    "    state('killPlace_over_maxPlace', False, e - s)\n",
    "    \n",
    "    state('healsandboosts')\n",
    "    s = timer()\n",
    "    df['healsandboosts'] = df['heals'] + df['boosts']\n",
    "    e = timer()                                  \n",
    "    state('healsandboosts', False, e - s)\n",
    "    \n",
    "    target = 'winPlacePerc'\n",
    "    features = list(df.columns)\n",
    "    \n",
    "    # Remove some features from the features list :\n",
    "    features.remove(\"Id\")\n",
    "    features.remove(\"matchId\")\n",
    "    features.remove(\"groupId\")\n",
    "    features.remove(\"matchDuration\")\n",
    "    features.remove(\"matchType\")\n",
    "    \n",
    "    y = None\n",
    "    if is_train: \n",
    "        y = np.array(df.groupby(['matchId', 'groupId'])[target].agg('mean'), dtype=np.float64)\n",
    "        # Remove the target from the features list :\n",
    "        features.remove(target)\n",
    "    \n",
    "    # Make new features indicating the mean of the features ( grouped by match and group ) :\n",
    "    print(\"get group mean feature\")\n",
    "    agg = df.groupby(['matchId', 'groupId'])[features].agg('mean')\n",
    "    agg_rank = agg.groupby(['matchId'])[features].rank(pct=True).reset_index()\n",
    "    \n",
    "    \n",
    "    # If we are processing the training data let df_out = the grouped  'matchId' and 'groupId'\n",
    "    if is_train: \n",
    "        df_out = agg.reset_index()[['matchId', 'groupId']]\n",
    "    else: \n",
    "        df_out = df[['matchId', 'groupId']]\n",
    "    \n",
    "    # Merge agg and agg_rank (that we got before) with df_out :\n",
    "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "    df_out = df_out.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    # Make new features indicating the median value of the features for each group ( grouped by match )\n",
    "    print(\"get group median feature\")\n",
    "    agg = df.groupby(['matchId','groupId'])[features].agg('median')\n",
    "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "    \n",
    "    # Merge the new (agg and agg_rank) with df_out :\n",
    "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "    df_out = df_out.merge(agg_rank, suffixes=[\"_median\", \"_median_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    # Make new features indicating the max value of the features for each group ( grouped by match )\n",
    "    print(\"get group max feature\")\n",
    "    agg = df.groupby(['matchId','groupId'])[features].agg('max')\n",
    "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "    \n",
    "    # Merge the new (agg and agg_rank) with df_out :\n",
    "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "    df_out = df_out.merge(agg_rank, suffixes=[\"_max\", \"_max_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    # Make new features indicating the minimum value of the features for each group ( grouped by match )\n",
    "    print(\"get group min feature\")\n",
    "    agg = df.groupby(['matchId','groupId'])[features].agg('min')\n",
    "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "    \n",
    "    # Merge the new (agg and agg_rank) with df_out :\n",
    "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "    df_out = df_out.merge(agg_rank, suffixes=[\"_min\", \"_min_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "     # Make new features indicating the sum value of the features for each group ( grouped by match )\n",
    "    print(\"get group max feature\")\n",
    "    agg = df.groupby(['matchId','groupId'])[features].agg('sum')\n",
    "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "    \n",
    "    # Merge the new (agg and agg_rank) with df_out :\n",
    "     print(\"get group sum feature\")\n",
    "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "    df_out = df_out.merge(agg_rank, suffixes=[\"_sum\", \"_sum_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    # Make new features indicating the number of players in each group ( grouped by match )\n",
    "    print(\"get group size feature\")\n",
    "    agg = df.groupby(['matchId','groupId']).size().reset_index(name='group_size')\n",
    "     \n",
    "    # Merge the group_size feature with df_out :\n",
    "    df_out = df_out.merge(agg, how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    # Make new features indicating the mean value of each features for each match :\n",
    "    print(\"get match mean feature\")\n",
    "    agg = df.groupby(['matchId'])[features].agg('mean').reset_index()\n",
    "    # Merge the new agg with df_out :\n",
    "    df_out = df_out.merge(agg, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\n",
    "    \n",
    "    # Make new features indicating the number of groups in each match :\n",
    "    print(\"get match size feature\")\n",
    "    agg = df.groupby(['matchId']).size().reset_index(name='match_size')\n",
    "    # Merge the match_size feature with df_out :\n",
    "    df_out = df_out.merge(agg, how='left', on=['matchId'])\n",
    "    \n",
    "    # Drop matchId and groupId\n",
    "    df_out.drop([\"matchId\", \"groupId\"], axis=1, inplace=True)\n",
    "    df_out = reduce_mem_usage(df_out)\n",
    "    \n",
    "    X = np.array(df_out, dtype=np.float64)\n",
    "    \n",
    "    del df, df_out, agg, agg_rank\n",
    "    gc.collect()\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-26T12:09:48.322435Z",
     "iopub.status.busy": "2022-04-26T12:09:48.321747Z",
     "iopub.status.idle": "2022-04-26T12:13:44.578005Z",
     "shell.execute_reply": "2022-04-26T12:13:44.57716Z",
     "shell.execute_reply.started": "2022-04-26T12:09:48.32238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on totalDistance ... \n",
      "Working on totalDistance took (0.016) Sec \n",
      "\n",
      "Working on killPlace_over_maxPlace ... \n",
      "Working on killPlace_over_maxPlace took (0.022) Sec \n",
      "\n",
      "Working on healsandboosts ... \n",
      "Working on healsandboosts took (0.005) Sec \n",
      "\n",
      "get group mean feature\n",
      "get group max feature\n",
      "get group min feature\n",
      "get group size feature\n",
      "get match mean feature\n",
      "get match size feature\n",
      "Memory usage of dataframe is 2017.90 MB --> 1124.92 MB (Decreased by 44.3%)\n",
      "Working on totalDistance ... \n",
      "Working on totalDistance took (0.011) Sec \n",
      "\n",
      "Working on killPlace_over_maxPlace ... \n",
      "Working on killPlace_over_maxPlace took (0.013) Sec \n",
      "\n",
      "Working on healsandboosts ... \n",
      "Working on healsandboosts took (0.003) Sec \n",
      "\n",
      "get group mean feature\n",
      "get group max feature\n",
      "get group min feature\n",
      "get group size feature\n",
      "get match mean feature\n",
      "get match size feature\n",
      "Memory usage of dataframe is 1925.73 MB --> 1071.70 MB (Decreased by 44.3%)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = feature_engineering(df_train,True)\n",
    "x_test, _ = feature_engineering(df_test,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-26T12:13:44.58032Z",
     "iopub.status.busy": "2022-04-26T12:13:44.579975Z",
     "iopub.status.idle": "2022-04-26T12:13:48.098605Z",
     "shell.execute_reply": "2022-04-26T12:13:48.097582Z",
     "shell.execute_reply.started": "2022-04-26T12:13:44.580262Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the train and the validation set for the fitting\n",
    "random_seed=1\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.05, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-26T12:13:48.100353Z",
     "iopub.status.busy": "2022-04-26T12:13:48.100037Z",
     "iopub.status.idle": "2022-04-26T12:13:48.108003Z",
     "shell.execute_reply": "2022-04-26T12:13:48.107131Z",
     "shell.execute_reply.started": "2022-04-26T12:13:48.100304Z"
    }
   },
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "RF = RandomForestRegressor(n_estimators=10, min_samples_leaf=3, max_features=0.5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-26T12:13:48.109939Z",
     "iopub.status.busy": "2022-04-26T12:13:48.109447Z",
     "iopub.status.idle": "2022-04-26T12:27:39.969403Z",
     "shell.execute_reply": "2022-04-26T12:27:39.968336Z",
     "shell.execute_reply.started": "2022-04-26T12:13:48.109827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 41s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_features=0.5, min_samples_leaf=3, n_estimators=10,\n",
       "                      n_jobs=-1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "RF.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-26T12:27:39.971521Z",
     "iopub.status.busy": "2022-04-26T12:27:39.970989Z",
     "iopub.status.idle": "2022-04-26T12:27:50.255155Z",
     "shell.execute_reply": "2022-04-26T12:27:50.25416Z",
     "shell.execute_reply.started": "2022-04-26T12:27:39.971462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae train RF:  0.015622325231198733\n",
      "mae val RF:  0.03291754968842933\n"
     ]
    }
   ],
   "source": [
    "mae_train_RF = mean_absolute_error(RF.predict(x_train), y_train)\n",
    "mae_val_RF = mean_absolute_error(RF.predict(x_val), y_val)\n",
    "print('mae train RF: ', mae_train_RF)\n",
    "print('mae val RF: ', mae_val_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-26T12:27:50.261387Z",
     "iopub.status.busy": "2022-04-26T12:27:50.259419Z",
     "iopub.status.idle": "2022-04-26T12:27:50.273851Z",
     "shell.execute_reply": "2022-04-26T12:27:50.272842Z",
     "shell.execute_reply.started": "2022-04-26T12:27:50.26133Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reference link: https://www.kaggle.com/chocozzz/lightgbm-baseline\n",
    "def run_lgb(train_X, train_y, val_X, val_y, x_test):\n",
    "    params = {\"objective\" : \"regression\", \n",
    "              \"metric\" : \"mae\", \n",
    "              'n_estimators':20000, \n",
    "              'early_stopping_rounds':200,\n",
    "              \"num_leaves\" : 31, \n",
    "              \"learning_rate\" : 0.05, \n",
    "              \"bagging_fraction\" : 0.7,\n",
    "              \"bagging_seed\" : 0, \n",
    "              \"num_threads\" : 4,\n",
    "              \"colsample_bytree\" : 0.7\n",
    "             }\n",
    "    \n",
    "    lgtrain = lgb.Dataset(train_X, label=train_y)\n",
    "    lgval = lgb.Dataset(val_X, label=val_y)\n",
    "    model = lgb.train(params, lgtrain, valid_sets=[lgtrain, lgval], early_stopping_rounds=200, verbose_eval=1000)\n",
    "    \n",
    "    pred_test_y = model.predict(x_test, num_iteration=model.best_iteration)\n",
    "    return pred_test_y, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-26T12:27:50.282248Z",
     "iopub.status.busy": "2022-04-26T12:27:50.279468Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tf\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "D:\\Anaconda3\\envs\\tf\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "D:\\Anaconda3\\envs\\tf\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.418383 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 32702\n",
      "[LightGBM] [Info] Number of data points in the train set: 1925406, number of used features: 170\n",
      "[LightGBM] [Info] Start training from score 0.499780\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[1000]\ttraining's l1: 0.0294591\tvalid_1's l1: 0.0298832\n",
      "[2000]\ttraining's l1: 0.0285622\tvalid_1's l1: 0.0293086\n",
      "[3000]\ttraining's l1: 0.0279197\tvalid_1's l1: 0.0289643\n",
      "[4000]\ttraining's l1: 0.0273968\tvalid_1's l1: 0.0287243\n",
      "[5000]\ttraining's l1: 0.0269249\tvalid_1's l1: 0.0285189\n",
      "[6000]\ttraining's l1: 0.0264933\tvalid_1's l1: 0.0283476\n",
      "[7000]\ttraining's l1: 0.0260874\tvalid_1's l1: 0.0282013\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training the model #\n",
    "pred_test_lgb, model = run_lgb(x_train, y_train, x_val, y_val, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_train_lgb = mean_absolute_error(model.predict(x_train, num_iteration=model.best_iteration), y_train)\n",
    "mae_val_lgb = mean_absolute_error(model.predict(x_val, num_iteration=model.best_iteration), y_val)\n",
    "\n",
    "print('mae train lgb: ', mae_train_lgb)\n",
    "print('mae val lgb: ', mae_val_lgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference link: https://www.kaggle.com/qingyuanwu/deep-neural-network\n",
    "def run_DNN(x_train, y_train, x_val, y_val, x_test):\n",
    "    NN_model = Sequential()\n",
    "    NN_model.add(Dense(x_train.shape[1],  input_dim = x_train.shape[1], activation='relu'))\n",
    "    NN_model.add(Dense(136, activation='relu'))\n",
    "    NN_model.add(Dense(136, activation='relu'))\n",
    "    NN_model.add(Dense(136, activation='relu'))\n",
    "    NN_model.add(Dense(136, activation='relu'))\n",
    "\n",
    "    # output Layer\n",
    "    NN_model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    # Compile the network :\n",
    "    NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "    NN_model.summary()\n",
    "    \n",
    "    checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "    checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    NN_model.fit(x=x_train, \n",
    "                 y=y_train, \n",
    "                 batch_size=1000,\n",
    "                 epochs=30, \n",
    "                 verbose=1, \n",
    "                 callbacks=callbacks_list,\n",
    "                 validation_split=0.15, \n",
    "                 validation_data=None, \n",
    "                 shuffle=True,\n",
    "                 class_weight=None, \n",
    "                 sample_weight=None, \n",
    "                 initial_epoch=0,\n",
    "                 steps_per_epoch=None, \n",
    "                 validation_steps=None)\n",
    "\n",
    "    pred_test_y = NN_model.predict(x_test)\n",
    "    pred_test_y = pred_test_y.reshape(-1)\n",
    "    return pred_test_y, NN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Training the model #\n",
    "pred_test_DNN, model = run_DNN(x_train, y_train, x_val, y_val, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_train_DNN = mean_absolute_error(model.predict(x_train), y_train)\n",
    "mae_val_DNN = mean_absolute_error(model.predict(x_val), y_val)\n",
    "print('mae train dnn: ', mae_train_DNN)\n",
    "print('mae val dnn: ', mae_val_DNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Use the model for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_RF = RF.predict(x_test)\n",
    "df_test['winPlacePerc_RF'] = pred_test_RF\n",
    "submission = df_test[['Id', 'winPlacePerc_RF']]\n",
    "submission.to_csv('../output/submission_RF.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['winPlacePerc_lgb'] = pred_test_lgb\n",
    "submission = df_test[['Id', 'winPlacePerc_lgb']]\n",
    "submission.to_csv('../output/submission_lgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['winPlacePerc_DNN'] = pred_test_DNN\n",
    "submission = df_test[['Id', 'winPlacePerc_DNN']]\n",
    "submission.to_csv('../output/submission_DNN.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ensembling(RF + DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_DNN = (1 - mae_val_DNN) / (3 - mae_val_DNN - mae_val_RF - mae_val_lgb)\n",
    "weight_RF = (1 - mae_val_RF) / (3 - mae_val_DNN - mae_val_RF - mae_val_lgb)\n",
    "weight_lgb = (1 - mae_val_lgb) / (3 - mae_val_DNN - mae_val_RF - mae_val_lgb)\n",
    "\n",
    "df_test['winPlacePerc'] = df_test.apply(lambda x: x['winPlacePerc_RF'] * weight_RF + x['winPlacePerc_DNN'] * weight_DNN + x['winPlacePerc_lgb'] * weight_lgb, axis=1)\n",
    "submission = df_test[['Id', 'winPlacePerc']]\n",
    "submission.to_csv('../output/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
