{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-04-26T12:09:06.595727Z",
     "iopub.status.busy": "2022-04-26T12:09:06.595338Z",
     "iopub.status.idle": "2022-04-26T12:09:12.963729Z",
     "shell.execute_reply": "2022-04-26T12:09:12.96286Z",
     "shell.execute_reply.started": "2022-04-26T12:09:06.59567Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2022-04-26T12:09:12.965925Z",
     "iopub.status.busy": "2022-04-26T12:09:12.965594Z",
     "iopub.status.idle": "2022-04-26T12:09:12.980682Z",
     "shell.execute_reply": "2022-04-26T12:09:12.979921Z",
     "shell.execute_reply.started": "2022-04-26T12:09:12.965827Z"
    }
   },
   "outputs": [],
   "source": [
    "# 通过类型转换节省内存空间\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB --> {:.2f} MB (Decreased by {:.1f}%)'.format(\n",
    "        start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-26T12:09:12.982522Z",
     "iopub.status.busy": "2022-04-26T12:09:12.981971Z",
     "iopub.status.idle": "2022-04-26T12:09:12.99351Z",
     "shell.execute_reply": "2022-04-26T12:09:12.992555Z",
     "shell.execute_reply.started": "2022-04-26T12:09:12.982449Z"
    }
   },
   "outputs": [],
   "source": [
    "def state(message,start = True, time = 0):\n",
    "    if(start):\n",
    "        print(f'Working on {message} ... ')\n",
    "    else :\n",
    "        print(f'Working on {message} took ({round(time , 3)}) Sec \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-26T12:09:12.995392Z",
     "iopub.status.busy": "2022-04-26T12:09:12.994892Z",
     "iopub.status.idle": "2022-04-26T12:09:47.075223Z",
     "shell.execute_reply": "2022-04-26T12:09:47.074342Z",
     "shell.execute_reply.started": "2022-04-26T12:09:12.995205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 983.90 MB --> 339.28 MB (Decreased by 65.5%)\n",
      "Memory usage of dataframe is 413.18 MB --> 140.19 MB (Decreased by 66.1%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assists</th>\n",
       "      <th>boosts</th>\n",
       "      <th>damageDealt</th>\n",
       "      <th>DBNOs</th>\n",
       "      <th>headshotKills</th>\n",
       "      <th>heals</th>\n",
       "      <th>killPlace</th>\n",
       "      <th>killPoints</th>\n",
       "      <th>kills</th>\n",
       "      <th>killStreaks</th>\n",
       "      <th>...</th>\n",
       "      <th>revives</th>\n",
       "      <th>rideDistance</th>\n",
       "      <th>roadKills</th>\n",
       "      <th>swimDistance</th>\n",
       "      <th>teamKills</th>\n",
       "      <th>vehicleDestroys</th>\n",
       "      <th>walkDistance</th>\n",
       "      <th>weaponsAcquired</th>\n",
       "      <th>winPoints</th>\n",
       "      <th>winPlacePerc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446966e+06</td>\n",
       "      <td>4.446965e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.338149e-01</td>\n",
       "      <td>1.106908e+00</td>\n",
       "      <td>1.307172e+02</td>\n",
       "      <td>6.578755e-01</td>\n",
       "      <td>2.268196e-01</td>\n",
       "      <td>1.370147e+00</td>\n",
       "      <td>4.759935e+01</td>\n",
       "      <td>5.050060e+02</td>\n",
       "      <td>9.247833e-01</td>\n",
       "      <td>5.439551e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.646590e-01</td>\n",
       "      <td>6.061160e+02</td>\n",
       "      <td>3.496091e-03</td>\n",
       "      <td>4.509323e+00</td>\n",
       "      <td>2.386841e-02</td>\n",
       "      <td>7.918208e-03</td>\n",
       "      <td>1.154218e+03</td>\n",
       "      <td>3.660488e+00</td>\n",
       "      <td>6.064601e+02</td>\n",
       "      <td>4.728218e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.885731e-01</td>\n",
       "      <td>1.715794e+00</td>\n",
       "      <td>1.707806e+02</td>\n",
       "      <td>1.145743e+00</td>\n",
       "      <td>6.021553e-01</td>\n",
       "      <td>2.679982e+00</td>\n",
       "      <td>2.746294e+01</td>\n",
       "      <td>6.275049e+02</td>\n",
       "      <td>1.558445e+00</td>\n",
       "      <td>7.109721e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>4.721671e-01</td>\n",
       "      <td>1.498344e+03</td>\n",
       "      <td>7.337297e-02</td>\n",
       "      <td>3.050220e+01</td>\n",
       "      <td>1.673935e-01</td>\n",
       "      <td>9.261157e-02</td>\n",
       "      <td>1.183497e+03</td>\n",
       "      <td>2.456544e+00</td>\n",
       "      <td>7.397004e+02</td>\n",
       "      <td>3.074050e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.400000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.551000e+02</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.424000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.700000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.856000e+02</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.583000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.860000e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>7.100000e+01</td>\n",
       "      <td>1.172000e+03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.909750e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.976000e+03</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.495000e+03</td>\n",
       "      <td>7.407000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.200000e+01</td>\n",
       "      <td>3.300000e+01</td>\n",
       "      <td>6.616000e+03</td>\n",
       "      <td>5.300000e+01</td>\n",
       "      <td>6.400000e+01</td>\n",
       "      <td>8.000000e+01</td>\n",
       "      <td>1.010000e+02</td>\n",
       "      <td>2.170000e+03</td>\n",
       "      <td>7.200000e+01</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.900000e+01</td>\n",
       "      <td>4.071000e+04</td>\n",
       "      <td>1.800000e+01</td>\n",
       "      <td>3.823000e+03</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>2.578000e+04</td>\n",
       "      <td>2.360000e+02</td>\n",
       "      <td>2.013000e+03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            assists        boosts   damageDealt         DBNOs  headshotKills  \\\n",
       "count  4.446966e+06  4.446966e+06  4.446966e+06  4.446966e+06   4.446966e+06   \n",
       "mean   2.338149e-01  1.106908e+00  1.307172e+02  6.578755e-01   2.268196e-01   \n",
       "std    5.885731e-01  1.715794e+00  1.707806e+02  1.145743e+00   6.021553e-01   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   0.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00  8.424000e+01  0.000000e+00   0.000000e+00   \n",
       "75%    0.000000e+00  2.000000e+00  1.860000e+02  1.000000e+00   0.000000e+00   \n",
       "max    2.200000e+01  3.300000e+01  6.616000e+03  5.300000e+01   6.400000e+01   \n",
       "\n",
       "              heals     killPlace    killPoints         kills   killStreaks  \\\n",
       "count  4.446966e+06  4.446966e+06  4.446966e+06  4.446966e+06  4.446966e+06   \n",
       "mean   1.370147e+00  4.759935e+01  5.050060e+02  9.247833e-01  5.439551e-01   \n",
       "std    2.679982e+00  2.746294e+01  6.275049e+02  1.558445e+00  7.109721e-01   \n",
       "min    0.000000e+00  1.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  2.400000e+01  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    0.000000e+00  4.700000e+01  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%    2.000000e+00  7.100000e+01  1.172000e+03  1.000000e+00  1.000000e+00   \n",
       "max    8.000000e+01  1.010000e+02  2.170000e+03  7.200000e+01  2.000000e+01   \n",
       "\n",
       "       ...       revives  rideDistance     roadKills  swimDistance  \\\n",
       "count  ...  4.446966e+06  4.446966e+06  4.446966e+06  4.446966e+06   \n",
       "mean   ...  1.646590e-01  6.061160e+02  3.496091e-03  4.509323e+00   \n",
       "std    ...  4.721671e-01  1.498344e+03  7.337297e-02  3.050220e+01   \n",
       "min    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%    ...  0.000000e+00  1.909750e-01  0.000000e+00  0.000000e+00   \n",
       "max    ...  3.900000e+01  4.071000e+04  1.800000e+01  3.823000e+03   \n",
       "\n",
       "          teamKills  vehicleDestroys  walkDistance  weaponsAcquired  \\\n",
       "count  4.446966e+06     4.446966e+06  4.446966e+06     4.446966e+06   \n",
       "mean   2.386841e-02     7.918208e-03  1.154218e+03     3.660488e+00   \n",
       "std    1.673935e-01     9.261157e-02  1.183497e+03     2.456544e+00   \n",
       "min    0.000000e+00     0.000000e+00  0.000000e+00     0.000000e+00   \n",
       "25%    0.000000e+00     0.000000e+00  1.551000e+02     2.000000e+00   \n",
       "50%    0.000000e+00     0.000000e+00  6.856000e+02     3.000000e+00   \n",
       "75%    0.000000e+00     0.000000e+00  1.976000e+03     5.000000e+00   \n",
       "max    1.200000e+01     5.000000e+00  2.578000e+04     2.360000e+02   \n",
       "\n",
       "          winPoints  winPlacePerc  \n",
       "count  4.446966e+06  4.446965e+06  \n",
       "mean   6.064601e+02  4.728218e-01  \n",
       "std    7.397004e+02  3.074050e-01  \n",
       "min    0.000000e+00  0.000000e+00  \n",
       "25%    0.000000e+00  2.000000e-01  \n",
       "50%    0.000000e+00  4.583000e-01  \n",
       "75%    1.495000e+03  7.407000e-01  \n",
       "max    2.013000e+03  1.000000e+00  \n",
       "\n",
       "[8 rows x 25 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataset\n",
    "df_train = pd.read_csv('../input/train_V2.csv')\n",
    "df_test = pd.read_csv('../input/test_V2.csv')\n",
    "\n",
    "# Reduce memory use\n",
    "df_train=reduce_mem_usage(df_train)\n",
    "df_test=reduce_mem_usage(df_test)\n",
    "\n",
    "# Show some data\n",
    "df_train.head()\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对数据进行简单清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-26T12:09:47.078981Z",
     "iopub.status.busy": "2022-04-26T12:09:47.078712Z",
     "iopub.status.idle": "2022-04-26T12:09:48.290097Z",
     "shell.execute_reply": "2022-04-26T12:09:48.289264Z",
     "shell.execute_reply.started": "2022-04-26T12:09:47.078924Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2      768836\n",
       "3      768347\n",
       "4      689622\n",
       "1      580951\n",
       "5      540721\n",
       "        ...  \n",
       "77          1\n",
       "75          1\n",
       "74          1\n",
       "71          1\n",
       "236         1\n",
       "Name: weaponsAcquired, Length: 97, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 由于百分比是按照本局的最差名次来计算的，而不是小队的数量，并且本局最差名次与小队数量存在冗余，因此删除\n",
    "# 由于最远击杀距离统计并不准确 rankPoints官方建议谨慎使用，因此删除\n",
    "df_train = df_train.drop(['longestKill', 'numGroups', 'rankPoints'], axis=1)\n",
    "df_test = df_test.drop(['longestKill', 'numGroups', 'rankPoints'], axis=1)\n",
    "\n",
    "# 删除缺失值\n",
    "df_train[df_train['winPlacePerc'].isnull()]\n",
    "df_train.drop(2744604, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-26T12:09:48.292767Z",
     "iopub.status.busy": "2022-04-26T12:09:48.292465Z",
     "iopub.status.idle": "2022-04-26T12:09:48.319923Z",
     "shell.execute_reply": "2022-04-26T12:09:48.319156Z",
     "shell.execute_reply.started": "2022-04-26T12:09:48.292717Z"
    }
   },
   "outputs": [],
   "source": [
    "def feature_engineering(df,is_train=True):\n",
    "    if is_train: \n",
    "        df = df[df['maxPlace'] > 1]\n",
    "\n",
    "    state('totalDistance')\n",
    "    s = timer()\n",
    "    df['totalDistance'] = df['rideDistance'] + df[\"walkDistance\"] + df[\"swimDistance\"]\n",
    "    e = timer()\n",
    "    state('totalDistance', False, e - s)\n",
    "          \n",
    "    state('killPlace_over_maxPlace')\n",
    "    s = timer()\n",
    "    df['killPlace_over_maxPlace'] = df['killPlace'] / df['maxPlace']\n",
    "    e = timer()                                  \n",
    "    state('killPlace_over_maxPlace', False, e - s)\n",
    "    \n",
    "    state('healsandboosts')\n",
    "    s = timer()\n",
    "    df['healsandboosts'] = df['heals'] + df['boosts']\n",
    "    e = timer()                                  \n",
    "    state('healsandboosts', False, e - s)\n",
    "    \n",
    "    target = 'winPlacePerc'\n",
    "    features = list(df.columns)\n",
    "    \n",
    "    # 去掉标称属性特征\n",
    "    features.remove(\"Id\")\n",
    "    features.remove(\"matchId\")\n",
    "    features.remove(\"groupId\")\n",
    "    features.remove(\"matchDuration\")\n",
    "    features.remove(\"matchType\")\n",
    "    \n",
    "    y = None\n",
    "    if is_train: \n",
    "        y = np.array(df.groupby(['matchId', 'groupId'])[target].agg('mean'), dtype=np.float64)\n",
    "        # 从特征中去掉百分比排名（预测目标）\n",
    "        features.remove(target)\n",
    "    \n",
    "    # 统计同场比赛中同组内的各个特征的平均值及其在该场比赛下的百分比\n",
    "    print(\"get group mean feature\")\n",
    "    agg = df.groupby(['matchId', 'groupId'])[features].agg('mean')\n",
    "    agg_rank = agg.groupby(['matchId'])[features].rank(pct=True).reset_index()\n",
    "    \n",
    "    \n",
    "    #创建一个以matchId和groupId为索引的新数据集\n",
    "    if is_train: \n",
    "        df_out = agg.reset_index()[['matchId', 'groupId']]\n",
    "    else: \n",
    "        df_out = df[['matchId', 'groupId']]\n",
    "    \n",
    "    # 将新特征与df_out根据matchId和groupId合并\n",
    "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "    df_out = df_out.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    # 统计同场比赛中同组内的各个特征的中值及其在该场比赛下的百分比\n",
    "    print(\"get group median feature\")\n",
    "    agg = df.groupby(['matchId','groupId'])[features].agg('median')\n",
    "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "    \n",
    "    # 将新特征与df_out根据matchId和groupId合并\n",
    "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "    df_out = df_out.merge(agg_rank, suffixes=[\"_median\", \"_median_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    # 统计同场比赛中同组内的各个特征的最大值及其在该场比赛下的百分比\n",
    "    print(\"get group max feature\")\n",
    "    agg = df.groupby(['matchId','groupId'])[features].agg('max')\n",
    "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "    \n",
    "    # 将新特征与df_out根据matchId和groupId合并\n",
    "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "    df_out = df_out.merge(agg_rank, suffixes=[\"_max\", \"_max_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    # 统计同场比赛中同组内的各个特征的最小值及其在该场比赛下的百分比\n",
    "    print(\"get group min feature\")\n",
    "    agg = df.groupby(['matchId','groupId'])[features].agg('min')\n",
    "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "    \n",
    "    # 将新特征与df_out根据matchId和groupId合并\n",
    "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "    df_out = df_out.merge(agg_rank, suffixes=[\"_min\", \"_min_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    # 统计同场比赛中同组内的各个特征的和及其在该场比赛下的百分比\n",
    "    print(\"get group max feature\")\n",
    "    agg = df.groupby(['matchId','groupId'])[features].agg('sum')\n",
    "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "    \n",
    "    # 将新特征与df_out根据matchId和groupId合并\n",
    "    print(\"get group sum feature\")\n",
    "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "    df_out = df_out.merge(agg_rank, suffixes=[\"_sum\", \"_sum_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    # 统计同场比赛中每个小组的人员数量\n",
    "    print(\"get group size feature\")\n",
    "    agg = df.groupby(['matchId','groupId']).size().reset_index(name='group_size')\n",
    "     \n",
    "    # 将Group_size特征与df_out根据matchId和groupId合并\n",
    "    df_out = df_out.merge(agg, how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    # 统计同场比赛下的特征平均值\n",
    "    print(\"get match mean feature\")\n",
    "    agg = df.groupby(['matchId'])[features].agg('mean').reset_index()\n",
    "    # 将新特征与df_out根据matchId合并\n",
    "    df_out = df_out.merge(agg, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\n",
    "    \n",
    "    # 统计同场比赛中小组数量\n",
    "    print(\"get match size feature\")\n",
    "    agg = df.groupby(['matchId']).size().reset_index(name='match_size')\n",
    "    # 将新特征与df_out根据matchId合并\n",
    "    df_out = df_out.merge(agg, how='left', on=['matchId'])\n",
    "    \n",
    "    # 删除matchId和groupId\n",
    "    df_out.drop([\"matchId\", \"groupId\"], axis=1, inplace=True)\n",
    "    df_out = reduce_mem_usage(df_out)\n",
    "    \n",
    "    X = np.array(df_out, dtype=np.float64)\n",
    "    \n",
    "    del df, df_out, agg, agg_rank\n",
    "    gc.collect()\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-26T12:09:48.322435Z",
     "iopub.status.busy": "2022-04-26T12:09:48.321747Z",
     "iopub.status.idle": "2022-04-26T12:13:44.578005Z",
     "shell.execute_reply": "2022-04-26T12:13:44.57716Z",
     "shell.execute_reply.started": "2022-04-26T12:09:48.32238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on totalDistance ... \n",
      "Working on totalDistance took (0.019) Sec \n",
      "\n",
      "Working on killPlace_over_maxPlace ... \n",
      "Working on killPlace_over_maxPlace took (0.032) Sec \n",
      "\n",
      "Working on healsandboosts ... \n",
      "Working on healsandboosts took (0.009) Sec \n",
      "\n",
      "get group mean feature\n",
      "get group median feature\n",
      "get group max feature\n",
      "get group min feature\n",
      "get group max feature\n",
      "get group sum feature\n",
      "get group size feature\n",
      "get match mean feature\n",
      "get match size feature\n",
      "Memory usage of dataframe is 3425.02 MB --> 1867.14 MB (Decreased by 45.5%)\n",
      "Working on totalDistance ... \n",
      "Working on totalDistance took (0.012) Sec \n",
      "\n",
      "Working on killPlace_over_maxPlace ... \n",
      "Working on killPlace_over_maxPlace took (0.016) Sec \n",
      "\n",
      "Working on healsandboosts ... \n",
      "Working on healsandboosts took (0.005) Sec \n",
      "\n",
      "get group mean feature\n",
      "get group median feature\n",
      "get group max feature\n",
      "get group min feature\n",
      "get group max feature\n",
      "get group sum feature\n",
      "get group size feature\n",
      "get match mean feature\n",
      "get match size feature\n",
      "Memory usage of dataframe is 3268.58 MB --> 1780.01 MB (Decreased by 45.5%)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = feature_engineering(df_train,True)\n",
    "x_test, _ = feature_engineering(df_test,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-26T12:13:44.58032Z",
     "iopub.status.busy": "2022-04-26T12:13:44.579975Z",
     "iopub.status.idle": "2022-04-26T12:13:48.098605Z",
     "shell.execute_reply": "2022-04-26T12:13:48.097582Z",
     "shell.execute_reply.started": "2022-04-26T12:13:44.580262Z"
    }
   },
   "outputs": [],
   "source": [
    "# 将数据集划分为训练集和验证集\n",
    "random_seed=1\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.05, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-26T12:13:48.100353Z",
     "iopub.status.busy": "2022-04-26T12:13:48.100037Z",
     "iopub.status.idle": "2022-04-26T12:13:48.108003Z",
     "shell.execute_reply": "2022-04-26T12:13:48.107131Z",
     "shell.execute_reply.started": "2022-04-26T12:13:48.100304Z"
    }
   },
   "outputs": [],
   "source": [
    "RF = RandomForestRegressor(n_estimators=10, min_samples_leaf=3, max_features=0.5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-26T12:13:48.109939Z",
     "iopub.status.busy": "2022-04-26T12:13:48.109447Z",
     "iopub.status.idle": "2022-04-26T12:27:39.969403Z",
     "shell.execute_reply": "2022-04-26T12:27:39.968336Z",
     "shell.execute_reply.started": "2022-04-26T12:13:48.109827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 26s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_features=0.5, min_samples_leaf=3, n_estimators=10,\n",
       "                      n_jobs=-1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "RF.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-26T12:27:39.971521Z",
     "iopub.status.busy": "2022-04-26T12:27:39.970989Z",
     "iopub.status.idle": "2022-04-26T12:27:50.255155Z",
     "shell.execute_reply": "2022-04-26T12:27:50.25416Z",
     "shell.execute_reply.started": "2022-04-26T12:27:39.971462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae train RF:  0.015503626984996665\n",
      "mae val RF:  0.032972121527289365\n"
     ]
    }
   ],
   "source": [
    "mae_train_RF = mean_absolute_error(RF.predict(x_train), y_train)\n",
    "mae_val_RF = mean_absolute_error(RF.predict(x_val), y_val)\n",
    "print('mae train RF: ', mae_train_RF)\n",
    "print('mae val RF: ', mae_val_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-26T12:27:50.261387Z",
     "iopub.status.busy": "2022-04-26T12:27:50.259419Z",
     "iopub.status.idle": "2022-04-26T12:27:50.273851Z",
     "shell.execute_reply": "2022-04-26T12:27:50.272842Z",
     "shell.execute_reply.started": "2022-04-26T12:27:50.26133Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_lgb(train_X, train_y, val_X, val_y, x_test):\n",
    "    params = {\"objective\" : \"regression\", \n",
    "              \"metric\" : \"mae\", \n",
    "              'n_estimators':20000, \n",
    "              'early_stopping_rounds':200,\n",
    "              \"num_leaves\" : 31, \n",
    "              \"learning_rate\" : 0.05, \n",
    "              \"bagging_fraction\" : 0.7,\n",
    "              \"bagging_seed\" : 0, \n",
    "              \"num_threads\" : 4,\n",
    "              \"colsample_bytree\" : 0.7\n",
    "             }\n",
    "    \n",
    "    lgtrain = lgb.Dataset(train_X, label=train_y)\n",
    "    lgval = lgb.Dataset(val_X, label=val_y)\n",
    "    model = lgb.train(params, lgtrain, valid_sets=[lgtrain, lgval], early_stopping_rounds=200, verbose_eval=1000)\n",
    "    \n",
    "    pred_test_y = model.predict(x_test, num_iteration=model.best_iteration)\n",
    "    return pred_test_y, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-26T12:27:50.282248Z",
     "iopub.status.busy": "2022-04-26T12:27:50.279468Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tf\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "D:\\Anaconda3\\envs\\tf\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "D:\\Anaconda3\\envs\\tf\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.753448 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 50691\n",
      "[LightGBM] [Info] Number of data points in the train set: 1925406, number of used features: 266\n",
      "[LightGBM] [Info] Start training from score 0.499780\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[1000]\ttraining's l1: 0.0293458\tvalid_1's l1: 0.0297825\n",
      "[2000]\ttraining's l1: 0.0284614\tvalid_1's l1: 0.0292488\n",
      "[3000]\ttraining's l1: 0.0278106\tvalid_1's l1: 0.0289249\n",
      "[4000]\ttraining's l1: 0.0272724\tvalid_1's l1: 0.02869\n",
      "[5000]\ttraining's l1: 0.0267899\tvalid_1's l1: 0.0284865\n",
      "[6000]\ttraining's l1: 0.0263591\tvalid_1's l1: 0.0283217\n",
      "[7000]\ttraining's l1: 0.0259541\tvalid_1's l1: 0.0281813\n",
      "[8000]\ttraining's l1: 0.0255732\tvalid_1's l1: 0.0280472\n",
      "[9000]\ttraining's l1: 0.0252231\tvalid_1's l1: 0.0279395\n",
      "[10000]\ttraining's l1: 0.0248689\tvalid_1's l1: 0.0278219\n",
      "[11000]\ttraining's l1: 0.0245357\tvalid_1's l1: 0.0277238\n",
      "[12000]\ttraining's l1: 0.0242178\tvalid_1's l1: 0.0276358\n",
      "[13000]\ttraining's l1: 0.023908\tvalid_1's l1: 0.027541\n",
      "[14000]\ttraining's l1: 0.0236167\tvalid_1's l1: 0.0274627\n",
      "[15000]\ttraining's l1: 0.0233332\tvalid_1's l1: 0.0273806\n",
      "[16000]\ttraining's l1: 0.0230589\tvalid_1's l1: 0.0273124\n",
      "[17000]\ttraining's l1: 0.0227881\tvalid_1's l1: 0.0272341\n",
      "[18000]\ttraining's l1: 0.022519\tvalid_1's l1: 0.0271576\n",
      "[19000]\ttraining's l1: 0.0222675\tvalid_1's l1: 0.0271\n",
      "[20000]\ttraining's l1: 0.0220168\tvalid_1's l1: 0.0270389\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20000]\ttraining's l1: 0.0220168\tvalid_1's l1: 0.0270389\n",
      "Wall time: 1h 32min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 训练模型\n",
    "pred_test_lgb, model = run_lgb(x_train, y_train, x_val, y_val, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae train lgb:  0.022016762514129016\n",
      "mae val lgb:  0.0270388973536143\n"
     ]
    }
   ],
   "source": [
    "mae_train_lgb = mean_absolute_error(model.predict(x_train, num_iteration=model.best_iteration), y_train)\n",
    "mae_val_lgb = mean_absolute_error(model.predict(x_val, num_iteration=model.best_iteration), y_val)\n",
    "\n",
    "print('mae train lgb: ', mae_train_lgb)\n",
    "print('mae val lgb: ', mae_val_lgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_DNN(x_train, y_train, x_val, y_val, x_test):\n",
    "    NN_model = Sequential()\n",
    "    NN_model.add(Dense(x_train.shape[1],  input_dim = x_train.shape[1], activation='relu'))\n",
    "    NN_model.add(Dense(136, activation='relu'))\n",
    "    NN_model.add(Dense(136, activation='relu'))\n",
    "    NN_model.add(Dense(136, activation='relu'))\n",
    "    NN_model.add(Dense(136, activation='relu'))\n",
    "\n",
    "    NN_model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "    NN_model.summary()\n",
    "    \n",
    "    checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "    checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    NN_model.fit(x=x_train, \n",
    "                 y=y_train, \n",
    "                 batch_size=1000,\n",
    "                 epochs=30, \n",
    "                 verbose=1, \n",
    "                 callbacks=callbacks_list,\n",
    "                 validation_split=0.15, \n",
    "                 validation_data=None, \n",
    "                 shuffle=True,\n",
    "                 class_weight=None, \n",
    "                 sample_weight=None, \n",
    "                 initial_epoch=0,\n",
    "                 steps_per_epoch=None, \n",
    "                 validation_steps=None)\n",
    "\n",
    "    pred_test_y = NN_model.predict(x_test)\n",
    "    pred_test_y = pred_test_y.reshape(-1)\n",
    "    return pred_test_y, NN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 266)               71022     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 136)               36312     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 136)               18632     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 136)               18632     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 136)               18632     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 137       \n",
      "=================================================================\n",
      "Total params: 163,367\n",
      "Trainable params: 163,367\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 1636595 samples, validate on 288811 samples\n",
      "Epoch 1/30\n",
      "1636595/1636595 [==============================] - 14s 9us/step - loss: 1.4802 - mean_absolute_error: 1.4802 - val_loss: 0.1726 - val_mean_absolute_error: 0.1726\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.17264, saving model to Weights-001--0.17264.hdf5\n",
      "Epoch 2/30\n",
      "1636595/1636595 [==============================] - 12s 8us/step - loss: 0.1296 - mean_absolute_error: 0.1296 - val_loss: 0.0984 - val_mean_absolute_error: 0.0984\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.17264 to 0.09842, saving model to Weights-002--0.09842.hdf5\n",
      "Epoch 3/30\n",
      "1636595/1636595 [==============================] - 13s 8us/step - loss: 0.0861 - mean_absolute_error: 0.0861 - val_loss: 0.0752 - val_mean_absolute_error: 0.0752\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.09842 to 0.07521, saving model to Weights-003--0.07521.hdf5\n",
      "Epoch 4/30\n",
      "1636595/1636595 [==============================] - 13s 8us/step - loss: 0.0749 - mean_absolute_error: 0.0749 - val_loss: 0.0657 - val_mean_absolute_error: 0.0657\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.07521 to 0.06569, saving model to Weights-004--0.06569.hdf5\n",
      "Epoch 5/30\n",
      "1636595/1636595 [==============================] - 13s 8us/step - loss: 0.0664 - mean_absolute_error: 0.0664 - val_loss: 0.0615 - val_mean_absolute_error: 0.0615\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.06569 to 0.06152, saving model to Weights-005--0.06152.hdf5\n",
      "Epoch 6/30\n",
      "1636595/1636595 [==============================] - 13s 8us/step - loss: 0.0618 - mean_absolute_error: 0.0618 - val_loss: 0.0573 - val_mean_absolute_error: 0.0573\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.06152 to 0.05730, saving model to Weights-006--0.05730.hdf5\n",
      "Epoch 7/30\n",
      "1636595/1636595 [==============================] - 13s 8us/step - loss: 0.0575 - mean_absolute_error: 0.0575 - val_loss: 0.0562 - val_mean_absolute_error: 0.0562\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.05730 to 0.05616, saving model to Weights-007--0.05616.hdf5\n",
      "Epoch 8/30\n",
      "1636595/1636595 [==============================] - 14s 8us/step - loss: 0.0548 - mean_absolute_error: 0.0548 - val_loss: 0.0519 - val_mean_absolute_error: 0.0519\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.05616 to 0.05193, saving model to Weights-008--0.05193.hdf5\n",
      "Epoch 9/30\n",
      "1636595/1636595 [==============================] - 13s 8us/step - loss: 0.0528 - mean_absolute_error: 0.0528 - val_loss: 0.0506 - val_mean_absolute_error: 0.0506\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.05193 to 0.05062, saving model to Weights-009--0.05062.hdf5\n",
      "Epoch 10/30\n",
      "1636595/1636595 [==============================] - 13s 8us/step - loss: 0.0512 - mean_absolute_error: 0.0512 - val_loss: 0.0559 - val_mean_absolute_error: 0.0559\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.05062\n",
      "Epoch 11/30\n",
      "1636595/1636595 [==============================] - 13s 8us/step - loss: 0.0498 - mean_absolute_error: 0.0498 - val_loss: 0.0510 - val_mean_absolute_error: 0.0510\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.05062\n",
      "Epoch 12/30\n",
      "1636595/1636595 [==============================] - 13s 8us/step - loss: 0.0489 - mean_absolute_error: 0.0489 - val_loss: 0.0488 - val_mean_absolute_error: 0.0488\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.05062 to 0.04885, saving model to Weights-012--0.04885.hdf5\n",
      "Epoch 13/30\n",
      "1636595/1636595 [==============================] - 14s 8us/step - loss: 0.0478 - mean_absolute_error: 0.0478 - val_loss: 0.0451 - val_mean_absolute_error: 0.0451\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.04885 to 0.04513, saving model to Weights-013--0.04513.hdf5\n",
      "Epoch 14/30\n",
      "1636595/1636595 [==============================] - 13s 8us/step - loss: 0.0470 - mean_absolute_error: 0.0470 - val_loss: 0.0457 - val_mean_absolute_error: 0.0457\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.04513\n",
      "Epoch 15/30\n",
      "1636595/1636595 [==============================] - 12s 8us/step - loss: 0.0464 - mean_absolute_error: 0.0464 - val_loss: 0.0466 - val_mean_absolute_error: 0.0466\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.04513\n",
      "Epoch 16/30\n",
      "1636595/1636595 [==============================] - 13s 8us/step - loss: 0.0459 - mean_absolute_error: 0.0459 - val_loss: 0.0481 - val_mean_absolute_error: 0.0481\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.04513\n",
      "Epoch 17/30\n",
      "1636595/1636595 [==============================] - 12s 8us/step - loss: 0.0457 - mean_absolute_error: 0.0457 - val_loss: 0.0467 - val_mean_absolute_error: 0.0467\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.04513\n",
      "Epoch 18/30\n",
      "1636595/1636595 [==============================] - 12s 8us/step - loss: 0.0450 - mean_absolute_error: 0.0450 - val_loss: 0.0451 - val_mean_absolute_error: 0.0451\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.04513 to 0.04509, saving model to Weights-018--0.04509.hdf5\n",
      "Epoch 19/30\n",
      "1636595/1636595 [==============================] - 13s 8us/step - loss: 0.0446 - mean_absolute_error: 0.0446 - val_loss: 0.0434 - val_mean_absolute_error: 0.0434\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.04509 to 0.04345, saving model to Weights-019--0.04345.hdf5\n",
      "Epoch 20/30\n",
      "1636595/1636595 [==============================] - 13s 8us/step - loss: 0.0443 - mean_absolute_error: 0.0443 - val_loss: 0.0421 - val_mean_absolute_error: 0.0421\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.04345 to 0.04206, saving model to Weights-020--0.04206.hdf5\n",
      "Epoch 21/30\n",
      "1636595/1636595 [==============================] - 12s 8us/step - loss: 0.0440 - mean_absolute_error: 0.0440 - val_loss: 0.0446 - val_mean_absolute_error: 0.0446\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.04206\n",
      "Epoch 22/30\n",
      "1636595/1636595 [==============================] - 13s 8us/step - loss: 0.0438 - mean_absolute_error: 0.0438 - val_loss: 0.0450 - val_mean_absolute_error: 0.0450\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.04206\n",
      "Epoch 23/30\n",
      "1636595/1636595 [==============================] - 12s 8us/step - loss: 0.0435 - mean_absolute_error: 0.0435 - val_loss: 0.0425 - val_mean_absolute_error: 0.0425\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.04206\n",
      "Epoch 24/30\n",
      "1636595/1636595 [==============================] - 12s 8us/step - loss: 0.0434 - mean_absolute_error: 0.0434 - val_loss: 0.0415 - val_mean_absolute_error: 0.0415\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.04206 to 0.04153, saving model to Weights-024--0.04153.hdf5\n",
      "Epoch 25/30\n",
      "1636595/1636595 [==============================] - 14s 8us/step - loss: 0.0431 - mean_absolute_error: 0.0431 - val_loss: 0.0481 - val_mean_absolute_error: 0.0481\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.04153\n",
      "Epoch 26/30\n",
      "1636595/1636595 [==============================] - 13s 8us/step - loss: 0.0431 - mean_absolute_error: 0.0431 - val_loss: 0.0416 - val_mean_absolute_error: 0.0416\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.04153\n",
      "Epoch 27/30\n",
      "1636595/1636595 [==============================] - 14s 8us/step - loss: 0.0427 - mean_absolute_error: 0.0427 - val_loss: 0.0446 - val_mean_absolute_error: 0.0446\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.04153\n",
      "Epoch 28/30\n",
      "1636595/1636595 [==============================] - 13s 8us/step - loss: 0.0426 - mean_absolute_error: 0.0426 - val_loss: 0.0428 - val_mean_absolute_error: 0.0428\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.04153\n",
      "Epoch 29/30\n",
      "1636595/1636595 [==============================] - 13s 8us/step - loss: 0.0425 - mean_absolute_error: 0.0425 - val_loss: 0.0456 - val_mean_absolute_error: 0.0456\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.04153\n",
      "Epoch 30/30\n",
      "1636595/1636595 [==============================] - 13s 8us/step - loss: 0.0423 - mean_absolute_error: 0.0423 - val_loss: 0.0410 - val_mean_absolute_error: 0.0410\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.04153 to 0.04104, saving model to Weights-030--0.04104.hdf5\n",
      "Wall time: 7min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 训练模型\n",
    "pred_test_DNN, model = run_DNN(x_train, y_train, x_val, y_val, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae train dnn:  0.04090054122667123\n",
      "mae val dnn:  0.040935060316103625\n"
     ]
    }
   ],
   "source": [
    "mae_train_DNN = mean_absolute_error(model.predict(x_train), y_train)\n",
    "mae_val_DNN = mean_absolute_error(model.predict(x_val), y_val)\n",
    "print('mae train dnn: ', mae_train_DNN)\n",
    "print('mae val dnn: ', mae_val_DNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  使用训练好的模型进行预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_RF = RF.predict(x_test)\n",
    "df_test['winPlacePerc_RF'] = pred_test_RF\n",
    "submission = df_test[['Id', 'winPlacePerc_RF']]\n",
    "submission.to_csv('../output/submission_RF.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['winPlacePerc_lgb'] = pred_test_lgb\n",
    "submission = df_test[['Id', 'winPlacePerc_lgb']]\n",
    "submission.to_csv('../output/submission_lgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['winPlacePerc_DNN'] = pred_test_DNN\n",
    "submission = df_test[['Id', 'winPlacePerc_DNN']]\n",
    "submission.to_csv('../output/submission_DNN.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 根据验证集上的MAE值为模型划分权重进行集成(RF + DNN + LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_DNN = (1 - mae_val_DNN) / (3 - mae_val_DNN - mae_val_RF - mae_val_lgb)\n",
    "weight_RF = (1 - mae_val_RF) / (3 - mae_val_DNN - mae_val_RF - mae_val_lgb)\n",
    "weight_lgb = (1 - mae_val_lgb) / (3 - mae_val_DNN - mae_val_RF - mae_val_lgb)\n",
    "\n",
    "df_test['winPlacePerc'] = df_test.apply(lambda x: x['winPlacePerc_RF'] * weight_RF + x['winPlacePerc_DNN'] * weight_DNN + x['winPlacePerc_lgb'] * weight_lgb, axis=1)\n",
    "submission = df_test[['Id', 'winPlacePerc']]\n",
    "submission.to_csv('../output/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
