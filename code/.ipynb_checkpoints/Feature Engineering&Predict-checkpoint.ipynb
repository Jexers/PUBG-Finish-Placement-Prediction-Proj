{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideas to experiment\n",
    "\n",
    "def original(df):\n",
    "    return df\n",
    "\n",
    "def items(df):\n",
    "    df['items'] = df['heals'] + df['boosts']\n",
    "    return df\n",
    "\n",
    "def players_in_team(df):\n",
    "    agg = df.groupby('groupId').size().to_frame('players_in_team')\n",
    "    df = df.merge(agg, on='groupId')\n",
    "    return df\n",
    "\n",
    "def total_distance(df):\n",
    "    df['total_distance'] = df['walkDistance'] + df['rideDistance'] + df['swimDistance']\n",
    "    return df\n",
    "\n",
    "def headshots_over_kills(df):\n",
    "    df['headshots_over_kills'] = df['headshotKills'] / df['kills']\n",
    "    df['headshots_over_kills'] = df['headshots_over_kills'].fillna(0)\n",
    "    return df\n",
    "\n",
    "def killPlace_over_maxPlace(df):\n",
    "    df['killPlace_over_maxPlace'] = df['killPlace'] / df['maxPlace']\n",
    "    return df\n",
    "\n",
    "def walkDistance_over_heals(df):\n",
    "    df['walkDistance_over_heals'] = df['walkDistance'] / df['heals']\n",
    "    df['walkDistance_over_heals'] = df['walkDistance_over_heals'].replace(np.inf, 0)\n",
    "    return df\n",
    "\n",
    "def walkDistance_over_kills(df):\n",
    "    df['workDistance_ove_kills'] = df['walkDistance'] / df['kills']\n",
    "    df['workDistance_ove_kills'] = df['workDistance_ove_kills'].fillna(0)\n",
    "    df['workDistance_ove_kills'] = df['workDistance_ove_kills'].replace(np.inf, 0)\n",
    "    return df\n",
    "\n",
    "def teamwork(df):\n",
    "    df['teamwork'] = df['assists'] + df['revives']\n",
    "    return df\n",
    "    \n",
    "def match_mode_classifier(mt):\n",
    "    if 'solo' in mt:\n",
    "        return 'solo'\n",
    "    elif 'duo' in mt:\n",
    "        return 'duo'\n",
    "    elif 'squad' in mt:\n",
    "        return 'squad'\n",
    "    else:\n",
    "        return 'others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My Idea to experiment\n",
    "def match_mode(df):\n",
    "    df['match_mode'] = df['matchType'].map(lambda mt: match_mode_classifier(mt))\n",
    "    dummies = pd.get_dummies(df['match_mode'])\n",
    "    df = df.join(dummies)\n",
    "    df = df.drop('match_mode', axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run experiments\n",
    "def run_experiments(functions):\n",
    "    results = []\n",
    "    for function in functions:\n",
    "        start = time.time()\n",
    "        score = run_experiment(function)\n",
    "        execution_time = time.time() - start\n",
    "        result = {\n",
    "            'name': function.__name__,\n",
    "            'score': score,\n",
    "            'exection time': f'{round(execution_time, 2)}s'\n",
    "        }\n",
    "        print(result)\n",
    "        results.append(result)\n",
    "        gc.collect()\n",
    "    return pd.DataFrame(results, columns=['name', 'score', 'execution time']).sort_values(by='score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run a experiment\n",
    "def run_experiment(function):\n",
    "    df = train.copy()\n",
    "    df = function(df)\n",
    "    \n",
    "    target = 'winPlacePerc'\n",
    "    cols_to_drop = ['Id', 'groupId', 'matchId', 'matchType', target]\n",
    "    cols_to_fit = [\n",
    "        col\n",
    "        for col in df.columns\n",
    "        if col not in cols_to_drop\n",
    "    ]\n",
    "    \n",
    "    X = df[cols_to_fit]\n",
    "    y = df[target].fillna(df[target].mean())\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)\n",
    "    \n",
    "    model = LGBMRegressor(random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_pred, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run Experiments!\n",
    "# run_experiments([\n",
    "#     teamwork,\n",
    "#     match_mode,\n",
    "#     original,\n",
    "#     items,\n",
    "#     players_in_team,\n",
    "#     total_distance,\n",
    "#     headshots_over_kills,\n",
    "#     killPlace_over_maxPlace,\n",
    "#     walkDistance_over_heals,\n",
    "#     walkDistance_over_kills,\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideas to experiment\n",
    "def min_by_team(df):\n",
    "    cols_to_drop = ['Id', 'groupId', 'matchId', 'matchType', 'winPlacePerc']\n",
    "    features = [col for col in df.columns if col not in cols_to_drop]\n",
    "    agg = df.groupby(['matchId','groupId'])[features].min()\n",
    "    return df.merge(agg, suffixes=['', '_min'], how='left', on=['matchId', 'groupId'])\n",
    "\n",
    "def max_by_team(df):\n",
    "    cols_to_drop = ['Id', 'groupId', 'matchId', 'matchType', 'winPlacePerc']\n",
    "    features = [col for col in df.columns if col not in cols_to_drop]\n",
    "    agg = df.groupby(['matchId', 'groupId'])[features].max()\n",
    "    return df.merge(agg, suffixes=['', '_max'], how='left', on=['matchId', 'groupId'])\n",
    "\n",
    "def sum_by_team(df):\n",
    "    cols_to_drop = ['Id', 'groupId', 'matchId', 'matchType', 'winPlacePerc']\n",
    "    features = [col for col in df.columns if col not in cols_to_drop]\n",
    "    agg = df.groupby(['matchId', 'groupId'])[features].sum()\n",
    "    return df.merge(agg, suffixes=['', '_sum'], how='left', on=['matchId', 'groupId'])\n",
    "\n",
    "def median_by_team(df):\n",
    "    cols_to_drop = ['Id', 'groupId', 'matchId', 'matchType', 'winPlacePerc']\n",
    "    features = [col for col in df.columns if col not in cols_to_drop]\n",
    "    agg = df.groupby(['matchId', 'groupId'])[features].median()\n",
    "    return df.merge(agg, suffixes=['', '_median'], how='left', on=['matchId', 'groupId'])\n",
    "\n",
    "def mean_by_team(df):\n",
    "    cols_to_drop = ['Id', 'groupId', 'matchId', 'matchType', 'winPlacePerc']\n",
    "    features = [col for col in df.columns if col not in cols_to_drop]\n",
    "    agg = df.groupby(['matchId', 'groupId'])[features].mean()\n",
    "    return df.merge(agg, suffixes=['', '_mean'], how='left', on=['matchId', 'groupId'])\n",
    "\n",
    "def rank_by_team(df):\n",
    "    cols_to_drop = ['Id', 'groupId', 'matchId', 'matchType', 'winPlacePerc']\n",
    "    features = [col for col in df.columns if col not in cols_to_drop]\n",
    "    agg = df.groupby(['matchId', 'groupId'])[features].mean()\n",
    "    agg = agg.groupby('matchId')[features].rank(pct=True)\n",
    "    return df.merge(agg, suffixes=['', '_mean_rank'], how='left', on=['matchId', 'groupId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run Experiments!\n",
    "# run_experiments([\n",
    "#     original,\n",
    "#     min_by_team,\n",
    "#     max_by_team,\n",
    "#     sum_by_team,\n",
    "#     median_by_team,\n",
    "#     mean_by_team,\n",
    "#     rank_by_team\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import eli5\n",
    "# from eli5.sklearn import PermutationImportance\n",
    "\n",
    "# target = 'winPlacePerc'\n",
    "# cols_to_drop = ['Id', 'groupId', 'matchId', 'matchType', target]\n",
    "# cols_to_fit = [col for col in train.columns if col not in cols_to_drop]\n",
    "\n",
    "# X = train[cols_to_fit]\n",
    "# y = train[target].fillna(train[target].mean())\n",
    "\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# model = LGBMRegressor(random_state=0)\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# perm = PermutationImportance(model, random_state=42).fit(X_valid, y_valid)\n",
    "# eli5.show_weights(perm, feature_names=list(cols_to_fit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_promising_preprocesses(df):\n",
    "    # Caution! There are dependencies to run.\n",
    "    df = run_promissing_aggregates(df)\n",
    "    df = run_promissing_conversions(df)\n",
    "    df = run_promissing_creations(df)\n",
    "    return df\n",
    "\n",
    "def run_promissing_aggregates(df):\n",
    "    # Common\n",
    "    cols_to_drop = ['Id', 'groupId', 'matchId', 'matchType', 'winPlacePerc']\n",
    "    features = [col for col in df.columns if col not in cols_to_drop]\n",
    "    \n",
    "    # Aggregates\n",
    "#     agg_min = df.groupby(['matchId','groupId'])[features].min()\n",
    "#     agg_max = df.groupby(['matchId', 'groupId'])[features].max()\n",
    "#     agg_sum = df.groupby(['matchId', 'groupId'])[features].sum()\n",
    "#     agg_median = df.groupby(['matchId', 'groupId'])[features].median()\n",
    "    agg_mean = df.groupby(['matchId', 'groupId'])[features].mean()\n",
    "    agg_rank = agg_mean.groupby('matchId')[features].rank(pct=True)\n",
    "    \n",
    "    # Merge\n",
    "#     df = df.merge(agg_min, suffixes=['', '_min'], how='left', on=['matchId', 'groupId'])\n",
    "#     df = df.merge(agg_max, suffixes=['', '_max'], how='left', on=['matchId', 'groupId'])\n",
    "#     df = df.merge(agg_sum, suffixes=['', '_sum'], how='left', on=['matchId', 'groupId'])\n",
    "#     df = df.merge(agg_median, suffixes=['', '_median'], how='left', on=['matchId', 'groupId'])\n",
    "#     df = df.merge(agg_mean, suffixes=['', '_mean'], how='left', on=['matchId', 'groupId'])\n",
    "    df = df.merge(agg_rank, suffixes=['', '_rank'], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def run_promissing_conversions(df):\n",
    "    df = match_mode(df) # One-Hot encoding 'matchMode'\n",
    "    return df\n",
    "\n",
    "def run_promissing_creations(df):\n",
    "    df = players_in_team(df) # Add 'players_in_team'\n",
    "    df = killPlace_over_maxPlace(df) # Add 'killPlace_over_maxPlace'\n",
    "    df = total_distance(df) # Add 'total_distance'\n",
    "    df = items(df) # Add 'total_distance'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Promising Preprocesses\n",
    "train_preprocessed = run_promising_preprocesses(train.copy())\n",
    "train_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "def evaluate(df):\n",
    "    target = 'winPlacePerc'\n",
    "    cols_to_drop = ['Id', 'groupId', 'matchId', 'matchType', target]\n",
    "    cols_to_fit = [col for col in df.columns if col not in cols_to_drop]\n",
    "    X = df[cols_to_fit]\n",
    "    y = df[target].fillna(df[target].mean())\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)\n",
    "\n",
    "    model = LGBMRegressor(random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_pred, y_valid)\n",
    "\n",
    "evaluate(train_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Memory Usage\n",
    "import sys\n",
    "\n",
    "print(\"{}{: >25}{}{: >10}{}\".format('|','Variable Name','|','Memory','|'))\n",
    "print(\" ------------------------------------ \")\n",
    "for var_name in dir():\n",
    "    if not var_name.startswith(\"_\"):\n",
    "        print(\"{}{: >25}{}{: >10}{}\".format('|',var_name,'|',sys.getsizeof(eval(var_name)),'|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting with promissing data\n",
    "target = 'winPlacePerc'\n",
    "cols_to_drop = ['Id', 'groupId', 'matchId', 'matchType', target]\n",
    "cols_to_fit = [col for col in train_preprocessed.columns if col not in cols_to_drop]\n",
    "\n",
    "X = train_preprocessed[cols_to_fit]\n",
    "y = train_preprocessed[target].fillna(train_preprocessed[target].mean())\n",
    "\n",
    "model = LGBMRegressor(random_state=0)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Test Data\n",
    "test = pd.read_csv('/kaggle/input/pubg-finish-placement-prediction/test_V2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Promising Preprocesses in Test\n",
    "test_preprocessed = run_promising_preprocesses(test.copy())\n",
    "test_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "X_test = test_preprocessed[cols_to_fit]\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'Id': test_preprocessed['Id'], # Caution!\n",
    "    'winPlacePerc': y_pred\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:experiment]",
   "language": "python",
   "name": "conda-env-experiment-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
